//this is part of the last step of the QA pipeline
//train a svm with feature-value pairs supplied by PrepareTrainNTest

package qa.main.ja

import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.classification.SVMModel
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.linalg.Vectors
import com.databricks.spark.csv
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.sql.types.{FloatType, IntegerType, StringType, StructField, StructType}
import scala.io.Source

// Load training data in LIBSVM format.


//This is part of the last step in the BOW QA pipeline
//ranking sentences in a doc


import java.io.{ FileWriter, BufferedWriter, File }

import org.apache.lucene.analysis.ja.JapaneseAnalyzer
import org.apache.lucene.index.{ IndexReader, Term, DirectoryReader }
import org.apache.lucene.queryparser.classic.QueryParser
import org.apache.lucene.search.{ TermQuery, IndexSearcher }
import org.apache.lucene.store.FSDirectory
import scala.math._
import scala.util.matching.Regex
import scala.xml._
import org.atilika.kuromoji.Tokenizer
import org.atilika.kuromoji.Token
import com.ibm.icu.text.Normalizer2



object TrainSpark {

  def prepareTestforParaAsDoc(questionXML: Node, indexDirName: String, testBw: BufferedWriter): Unit = {
    val sectIndexDir = FSDirectory.open(new File(indexDirName + "/sect"))
    val paraIndexDir = FSDirectory.open(new File(indexDirName + "/para"))
    val sentIndexDir = FSDirectory.open(new File(indexDirName + "/sent"))
    val sectIndexReader = DirectoryReader.open(sectIndexDir)
    val sectIndexSearcher = new IndexSearcher(sectIndexReader)
    val paraIndexReader = DirectoryReader.open(paraIndexDir)
    val paraIndexSearcher = new IndexSearcher(paraIndexReader)
    val sentIndexReader = DirectoryReader.open(sentIndexDir)
    val sentIndexSearcher = new IndexSearcher(sentIndexReader)
    for (doc <- (questionXML \\ "doc")) yield {
      val sectSectNum = sectSect((doc \\ "did").text)
      val sectParaNum = sectParaFrParaId((doc \\ "did").text)
      System.err.println(s"sectParaOut ${sectParaNum}")


      val paraIDRe = """r\d+""".r
      val idWoPara = paraIDRe.replaceAllIn((doc \\ "did").text, "")
      //idWoPara = 2:450-4-1-1-5
      val sectIDRe = """(?<=s)(\d+)(?=[\D]*)$""".r
      val sectIDwSepRe = """s(\d+)(?=[\D]*)$""".r
      val splitIDOpt = sectIDRe.findFirstIn(idWoPara.trim())
      //splitIDOpt = Some(5)
      val idWoLastSect = sectIDwSepRe.replaceFirstIn(idWoPara.trim(), "")
      val paraFrTop = splitIDOpt match {
        case None => {
          sectParaNum.toString()
          //            sectParaNum.toString() ::  List(label.toString())
        }
        case Some(splitID) => {
          if ((splitID.toInt == 0) == false) {
            System.err.println(s"splitIDInCheckQ&A ${splitID}")
            sectParaNum + getParasOfPrev(splitID.toInt - 1, idWoLastSect, sectIndexSearcher) + wholePara(idWoLastSect, sectIndexSearcher)
          } else 0
        }
      }
      System.err.println(s"idWoParaInCheckQ&A ${idWoPara}")
      System.err.println(s"idWoLastSectInCheckQ&A ${idWoLastSect}")
      System.err.println(s"splitIDinCheckQ&A ${splitIDOpt}")
      //        for (answer <- questionXML \\ "answer") yield {
      //          if (normalizedText.contains(myNormalize(answer.text.trim()))) println ("get")
      //        }
      //        val label = "false"
      val sentenceCounterStream = Stream.iterate(1)(_ + 1).iterator
      for (sentence <- (doc \\ "sent")) yield {
        //          System.err.println(s"doc ${doc}")val data = MLUtils.loadLibSVMFile(sc, "~/iGlobal/output/training.libsvm")
        val sentText = SharedFunctions.myNormalize((sentence \\ "stext").text)
        val sentenceCounter = sentenceCounterStream.next
        //          val sentenceID = (doc \\ "did").text+"^"+sentenceCounter
        //          System.err.println(s"sentenceCounter ${sentenceCounter}")

        val label = {
          if ((questionXML \\ "answer") exists { a: Node => sentText.contains(SharedFunctions.myNormalize(a.text.trim())) }) 1 else 0
        }
        System.err.println(s"sentence in checkQ&A${sentence}")
        //          for (answer<-(questionXML \\ "answer")) System.err.println(s"answer in checkQ&A${myNormalize(answer.text.trim())}")
        //          val maxProximitySentence  = ((doc \\ "dtext").text.trim().split("ã€‚"))maxBy(proximity(_, (questionXML\"text").text, index_reader))
        val sentenceProximity = proximity(sentText, (questionXML \ "text").text, sentIndexReader)

        val sentenceNum = (sentence \\ "scount").text.trim()
        val featLast1 = sectParaNum.toString() :: List(label)

        //idWoLastSect = 2:450-4-1-1
        val featLast2 = paraFrTop :: featLast1
        //            else 0 :: List(label.toString())
        val featLast3 = (doc \\ "dscore").text.trim() :: featLast2
        val featLast4 = sentenceProximity :: featLast3
        val featLast5 = sectSectNum :: featLast4
        val featLast6 = (sentence \\ "sscore").text.trim() :: featLast5
        val featLast7 = sentenceNum :: featLast6
        val testLine = (questionXML \ "@id").text.trim() :: (doc \\ "did").text.trim() + "t" + sentenceCounter :: featLast7
        testBw.write(testLine.mkString(",") + "\n")

      }
    }
  }

  
  def makeFeatList(cols: List[String]): List = {
    @tailrec
	val numberStream = Stream.from(0).iterator
    def featAccumulator(cols: List[String], featList: List[String]): List[String] = {
      ints match {
        case Nil => featList
        case x :: tail => featAccumulator(tail, featList)
      }
    }
	val counter = numberStream.next
    featAccumulator(cols, (counter+1).toString.concat(":") :: cols[counter]))
  }
  
  
  

  def prepareTrain(trainSrc: scala.io.BufferedSource, trainBw: BufferedWriter): Unit = {
    for (line <- trainSrc.getLines) {
	  	
      val cols = line.split(",").map(_.trim)
	  
      val featLast1 = "25:" :: cols[25-numberStream.next])

        //idWoLastSect = 2:450-4-1-1
        val featLast2 = "6:" :: paraFrTop :: " " :: featLast1
        //            else 0 :: List(label.toString())
        val featLast3 = "5:" :: (doc \\ "dscore").text.trim() :: " " :: featLast2
        val featLast4 = "4:" :: sentenceProximity :: " " :: featLast3
        val featLast5 = "3:" :: sectSectNum :: " " :: featLast4
        val featLast6 = "2:" :: (sentence \\ "sscore").text.trim() :: " " :: featLast5
        val featLast7 = "1:" :: sentenceNum :: " " :: featLast6
        val allFeats = featLast7.mkString
        trainBw.write(label.toString() + " " + allFeats + "\n")
    }
  }


  def answerSelNEval(model: SVMModel, df: org.apache.spark.sql.DataFrame) = {
    val maxScoreRow = df.collect().maxBy(r =>
    {
      //for paraAsDoc
      val features = Vectors.dense(r.getInt(2), r.getFloat(3), r.getInt(4), r.getFloat(5), r.getFloat(6), r.getInt(7), r.getInt(8))
      //for sectAsDoc
      //        val features = Vectors.dense(r.getInt(2), r.getFloat(3), r.getInt(4), r.getFloat(5), r.getFloat(6))
      model.predict(features)
    })
    //for paraAsDoc
    maxScoreRow.getInt(7)
    //for sectAsDoc
    //    maxScoreRow.getInt(7)
  }

  //ceiling =  correct answers found in the docs
  //notice that some answers cannot not be found in the wiki docs

  def getCeiling(df: org.apache.spark.sql.DataFrame) = {
    //    for paraAsDoc
    val foundOrNot = df.collect().exists(r => r.getInt(9) == 1)
    //  for sectAsDoc
    //    val foundOrNot = df.collect().exists(r => r.getInt(7) == 1)
    if (foundOrNot) 1 else 0
  }

  def ifAtAll(qIds: Array[String], df: org.apache.spark.sql.DataFrame) = {
    qIds.map(s => getCeiling(df.filter(s"""qId = "$s""""))).sum
  }


  //return best 1 sentence
  def best1(model: SVMModel, qIds: Array[String], df: org.apache.spark.sql.DataFrame) = {
    //    import sqlContext.implicits._
    //    val needQId = "NIILC-ECQA2015-00903-01"

    //    df.filter(s"""qId = "$needQId"""").show()
    qIds.map(s => answerSelNEval(model, df.filter(s"""qId = "$s""""))).sum
  }

  def main(args: Array[String]): Unit = {


    val conf = new SparkConf().setAppName("Simple Application").setMaster("local")
    val sc = new SparkContext(conf)
    val sqlContext = new org.apache.spark.sql.SQLContext(sc)
    // Load training data in LIBSVM format.
    val trainFile = new File("~/iGlobal/output/training.libsvm")
    val trainBw = new BufferedWriter(new FileWriter(trainFile))
    val testFile = new File("~/iGlobal/output/test.csv")
    val testBw = new BufferedWriter(new FileWriter(testFile))



    // Split data into training (60%) and test (40%).
    //   val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)
    //   val training = splits(0).cache()
    //   val test = splits(1)

    // Run training algorithm to build the model
    val numIterations = 100
    val model = SVMWithSGD.train(data, numIterations)


    val bufferedTrainSrc = scala.io.Source.fromFile("~/iGlobal/input/competencies_4.csv")
    for (line <- bufferedTrainSrc.getLines) {
      val cols = line.split(",").map(_.trim)
   // do whatever you want with the columns here
//      println(s"${cols(0)}|${cols(1)}|${cols(2)}|${cols(3)}")
    }
    bufferedTrainSrc.close

    prepareTrain(bufferedTrainSrc,trainBw)

    // Clear the default threshold.

    val data = MLUtils.loadLibSVMFile(sc, "~/iGlobal/output/training.libsvm")

    model.clearThreshold()
    /**schema for paraAsDoc
      */
    val customSchema = StructType(Array(
      StructField("qId", StringType, true),
      StructField("sentenceId", StringType, true),
      StructField("sentenceNum", IntegerType, true),
      StructField("sentence Lucene score", FloatType, true),
      StructField("sectionNum", IntegerType, true),
      StructField("proximity", FloatType, true),
      StructField("doc Lucene score", FloatType, true),
      StructField("paraNum fr Top", IntegerType, true),
      StructField("paraNum", IntegerType, true),
      StructField("label", IntegerType, true)
    ))

    /**
    //schema for sectAsDoc
    val customSchema = StructType(Array(
      StructField("qId", StringType, true),
      StructField("sentenceId", StringType, true),
      StructField("sentenceNum", IntegerType, true),
      StructField("sentence Lucene score", FloatType, true),
      StructField("sectionNum", IntegerType, true),
      StructField("proximity", FloatType, true),
      StructField("doc Lucene score", FloatType, true),
      StructField("label", IntegerType, true)
    ))
      */
    val df = sqlContext.read
      .format("com.databricks.spark.csv")
      .option("header", "false") // Use first line of all files as header
      .schema(customSchema)
      .load("/home/wailoktam/qa/mylab/test.csv")

    val qIdRows = df.select("qId").collect().distinct
    val qIds = qIdRows.map(_.getString(0))
    System.err.println("weights" + model.weights.toString())
    System.err.println("qIds" + qIds.length)
    System.err.println("ifAtAll" + ifAtAll(qIds, df))
    System.err.println("best1" + best1(model, qIds, df))

    // Compute raw scores on the test set.
    /**
      * val scoreAndLabels = test.map { point =>
      * val score = model.predict(point.features)
      * System.err.println(score)
      * (score, point.label)
      * }
      *
      * // Get evaluation metrics.
      * val metrics = new BinaryClassificationMetrics(scoreAndLabels)
      * val auROC = metrics.areaUnderROC()
      *
      * println("Area under ROC = " + auROC)
      *
      * // Save and load model
      * model.save(sc, "myModelPath")
      * val sameModel = SVMModel.load(sc, "myModelPath")
      */

    /**
      *
      * val conf = new SparkConf().setAppName("Simple Application").setMaster("local");
      * val sc = new SparkContext(conf)
      * val sqlContext = new org.apache.spark.sql.SQLContext(sc)
      * val data = MLUtils.loadLibSVMFile(sc, "/home/wailoktam/qa/mylab/input/training.csv")
      *
      * // Split data into training (60%) and test (40%).
      * val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)
      * val training = splits(0).cache()
      * val test = splits(1)
      *
      * // Run training algorithm to build the model
      * val numIterations = 100
      * val model = SVMWithSGD.train(training, numIterations)
      *
      * // Clear the default threshold.
      * model.clearThreshold()
      *
      * // Compute raw scores on the test set.
      * val scoreAndLabels = test.map { point =>
      * val score = model.predict(point.features)
      * (score, point.label)
      * }
      *
      * // Get evaluation metrics.
      * val metrics = new BinaryClassificationMetrics(scoreAndLabels)
      * val auROC = metrics.areaUnderROC()
      *
      * println("Area under ROC = " + auROC)
      */
  }

}
